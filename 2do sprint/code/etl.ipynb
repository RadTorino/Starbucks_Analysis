{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL de la Data Business Starbucks Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def procesar_y_guardar_starbucks(pkl_path, save_directory):\n",
    "    # Cargar el archivo .pkl en un DataFrame de Pandas\n",
    "    df_business_starbucks = pd.read_pickle(pkl_path)\n",
    "\n",
    "    # Eliminar las columnas duplicadas\n",
    "    df_business_starbucks = df_business_starbucks.loc[:, ~df_business_starbucks.columns.duplicated()]\n",
    "\n",
    "    # Filtrar el DataFrame para incluir solo las filas donde 'name' contiene la palabra 'Starbucks'\n",
    "    df_business_starbucks = df_business_starbucks[df_business_starbucks['name'].str.contains('Starbucks', na=False)]\n",
    "\n",
    "    # Crear la nueva columna 'starbucks_id' concatenando las columnas especificadas, separadas por un guion\n",
    "    df_business_starbucks['starbucks_id'] = df_business_starbucks['state'].astype(str) + '-' + \\\n",
    "                                   df_business_starbucks['latitude'].astype(str) + '-' + \\\n",
    "                                   df_business_starbucks['longitude'].astype(str)\n",
    "\n",
    "    # Convertir la columna 'postal_code' a tipo numérico (float)\n",
    "    df_business_starbucks['postal_code'] = pd.to_numeric(df_business_starbucks['postal_code'], errors='coerce')\n",
    "\n",
    "    # Convertir la columna 'city' a tipo str\n",
    "    df_business_starbucks['city'] = df_business_starbucks['city'].astype(str)\n",
    "\n",
    "    # Eliminar las filas donde la columna 'name' tiene valores NaN\n",
    "    df_business_starbucks = df_business_starbucks.dropna(subset=['name'])\n",
    "\n",
    "    # Eliminar las columnas 'attributes' y 'hours'\n",
    "    df_business_starbucks.drop(columns=['attributes', 'hours'], inplace=True)\n",
    "\n",
    "    # Restablecer el índice\n",
    "    df_business_starbucks = df_business_starbucks.reset_index(drop=True)\n",
    "\n",
    "    # Obtener la fecha actual en formato YYYY-MM-DD\n",
    "    fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Crear el nombre del archivo\n",
    "    nombre_archivo = f'business_starbucks_{fecha_actual}.parquet'\n",
    "\n",
    "    # Guardar el DataFrame como archivo Parquet en el directorio especificado\n",
    "    df_business_starbucks.to_parquet(f'{save_directory}/{nombre_archivo}')\n",
    "\n",
    "    def guardar_y_comparar(df):\n",
    "        # Obtener la fecha actual en formato YYYY-MM-DD\n",
    "        fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "        # Crear el nombre del archivo\n",
    "        nombre_archivo = f'business_starbucks_{fecha_actual}.parquet'\n",
    "        # Guardar el DataFrame como archivo Parquet en el directorio especificado\n",
    "        df.to_parquet(f'{save_directory}/{nombre_archivo}')\n",
    "\n",
    "        # Comprobar si el archivo existe\n",
    "        archivo_antiguo = f'{save_directory}/business_starbucks.parquet'\n",
    "        if os.path.exists(archivo_antiguo):\n",
    "            # Cargar el archivo existente\n",
    "            df_antiguo = pd.read_parquet(archivo_antiguo)\n",
    "            \n",
    "            # Concatenar los datos que no están presentes en el archivo existente\n",
    "            df_nuevo = pd.concat([df_antiguo, df]).drop_duplicates().reset_index(drop=True)\n",
    "            \n",
    "            # Guardar el DataFrame actualizado con el mismo nombre\n",
    "            df_nuevo.to_parquet(archivo_antiguo)\n",
    "            \n",
    "            # Verificar si se actualizó la data\n",
    "            if len(df) > 0:\n",
    "                print(\"Se ha actualizado la data.\")\n",
    "            else:\n",
    "                print(\"No hay nuevos datos para actualizar.\")\n",
    "        else:\n",
    "            print(\"No se encontró el archivo antiguo. Guardando el nuevo archivo.\")\n",
    "\n",
    "    # Usar la función con el DataFrame procesado\n",
    "    guardar_y_comparar(df_business_starbucks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha actualizado la data.\n"
     ]
    }
   ],
   "source": [
    "# Ejecucion de la Funcion\n",
    "procesar_y_guardar_starbucks('../data/business.pkl', '../gcp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL de la Data Business Dunkin Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def procesar_y_guardar_dunkin(pkl_path, save_directory):\n",
    "    # Cargar el archivo .pkl en un DataFrame de Pandas\n",
    "    df_business_dunkin = pd.read_pickle(pkl_path)\n",
    "\n",
    "    # Eliminar las columnas duplicadas\n",
    "    df_business_dunkin = df_business_dunkin.loc[:, ~df_business_dunkin.columns.duplicated()]\n",
    "\n",
    "    # Filtrar el DataFrame para incluir solo las filas donde 'name' contiene la palabra 'Dunkin'\n",
    "    df_business_dunkin = df_business_dunkin[df_business_dunkin['name'].str.contains('Dunkin', na=False)]\n",
    "\n",
    "    # Crear la nueva columna 'dunkin_id' concatenando las columnas especificadas, separadas por un guion\n",
    "    df_business_dunkin['dunkin_id'] = df_business_dunkin['state'].astype(str) + '-' + \\\n",
    "                                   df_business_dunkin['latitude'].astype(str) + '-' + \\\n",
    "                                   df_business_dunkin['longitude'].astype(str)\n",
    "\n",
    "    # Convertir la columna 'postal_code' a tipo numérico (float)\n",
    "    df_business_dunkin['postal_code'] = pd.to_numeric(df_business_dunkin['postal_code'], errors='coerce')\n",
    "\n",
    "    # Convertir la columna 'city' a tipo str\n",
    "    df_business_dunkin['city'] = df_business_dunkin['city'].astype(str)\n",
    "\n",
    "    # Eliminar las filas donde la columna 'name' tiene valores NaN\n",
    "    df_business_dunkin = df_business_dunkin.dropna(subset=['name'])\n",
    "\n",
    "    # Eliminar las columnas 'attributes' y 'hours'\n",
    "    df_business_dunkin.drop(columns=['attributes', 'hours'], inplace=True)\n",
    "\n",
    "    # Restablecer el índice\n",
    "    df_business_dunkin = df_business_dunkin.reset_index(drop=True)\n",
    "\n",
    "    # Obtener la fecha actual en formato YYYY-MM-DD\n",
    "    fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Crear el nombre del archivo\n",
    "    nombre_archivo = f'business_dunkin_{fecha_actual}.parquet'\n",
    "\n",
    "    # Guardar el DataFrame como archivo Parquet en el directorio especificado\n",
    "    df_business_dunkin.to_parquet(f'{save_directory}/{nombre_archivo}')\n",
    "\n",
    "    def guardar_y_comparar(df):\n",
    "        # Obtener la fecha actual en formato YYYY-MM-DD\n",
    "        fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "        # Crear el nombre del archivo\n",
    "        nombre_archivo = f'business_dunkin_{fecha_actual}.parquet'\n",
    "        # Guardar el DataFrame como archivo Parquet en el directorio especificado\n",
    "        df.to_parquet(f'{save_directory}/{nombre_archivo}')\n",
    "\n",
    "        # Comprobar si el archivo existe\n",
    "        archivo_antiguo = f'{save_directory}/business_dunkin.parquet'\n",
    "        if os.path.exists(archivo_antiguo):\n",
    "            # Cargar el archivo existente\n",
    "            df_antiguo = pd.read_parquet(archivo_antiguo)\n",
    "            \n",
    "            # Concatenar los datos que no están presentes en el archivo existente\n",
    "            df_nuevo = pd.concat([df_antiguo, df]).drop_duplicates().reset_index(drop=True)\n",
    "            \n",
    "            # Guardar el DataFrame actualizado con el mismo nombre\n",
    "            df_nuevo.to_parquet(archivo_antiguo)\n",
    "            \n",
    "            # Verificar si se actualizó la data\n",
    "            if len(df) > 0:\n",
    "                print(\"Se ha actualizado la data.\")\n",
    "            else:\n",
    "                print(\"No hay nuevos datos para actualizar.\")\n",
    "        else:\n",
    "            print(\"No se encontró el archivo antiguo. Guardando el nuevo archivo.\")\n",
    "\n",
    "    # Usar la función con el DataFrame procesado\n",
    "    guardar_y_comparar(df_business_dunkin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecucion de la Funcion\n",
    "procesar_y_guardar_dunkin('../data/business.pkl', '../gcp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL de la Data Review Starbucks Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def process_reviews_starbucks():\n",
    "    # Directorios y archivos\n",
    "    review_file_path = '../data/reviews.json'\n",
    "    business_file_path = '../gcp/business_starbucks.parquet'\n",
    "    output_dir = '../gcp/'\n",
    "    output_file_base = 'reviews_starbucks'\n",
    "    output_file_ext = '.parquet'\n",
    "    \n",
    "    # Leer el archivo reviews.json entero\n",
    "    with open(review_file_path, 'r') as f:\n",
    "        reviews = json.load(f)\n",
    "    df_reviews = pd.DataFrame(reviews)\n",
    "    \n",
    "    # Leer el archivo business_starbucks.parquet\n",
    "    df_business = pd.read_parquet(business_file_path)\n",
    "    \n",
    "    # Verificar que las columnas necesarias existan\n",
    "    if 'business_id' not in df_reviews.columns:\n",
    "        raise KeyError(\"La columna 'business_id' no se encuentra en el archivo reviews.json\")\n",
    "    if 'business_id' not in df_business.columns:\n",
    "        raise KeyError(\"La columna 'business_id' no se encuentra en el archivo business_starbucks.parquet\")\n",
    "    \n",
    "    # Cruzar ambos archivos por la columna business_id\n",
    "    df_reviews_starbucks = df_reviews[df_reviews['business_id'].isin(df_business['business_id'])]\n",
    "    \n",
    "    # Verificar que la columna 'date' exista\n",
    "    if 'date' not in df_reviews_starbucks.columns:\n",
    "        raise KeyError(\"La columna 'date' no se encuentra en el DataFrame resultante después del cruce\")\n",
    "    \n",
    "    # Eliminar la hora de la columna 'date', manteniendo el tipo datetime\n",
    "    df_reviews_starbucks['date'] = pd.to_datetime(df_reviews_starbucks['date']).dt.normalize()\n",
    "    \n",
    "    # Reiniciar el índice del DataFrame\n",
    "    df_reviews_starbucks.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Convertir a parquet sin guardar aún\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    temp_output_file = os.path.join(output_dir, f\"{output_file_base}_{current_date}{output_file_ext}\")\n",
    "    df_reviews_starbucks.to_parquet(temp_output_file)\n",
    "    \n",
    "    final_output_file = os.path.join(output_dir, f\"{output_file_base}{output_file_ext}\")\n",
    "    \n",
    "    # Si el archivo reviews_starbucks.parquet ya existe\n",
    "    if os.path.exists(final_output_file):\n",
    "        existing_reviews = pd.read_parquet(final_output_file)\n",
    "        \n",
    "        # Combinar DataFrames y eliminar duplicados\n",
    "        combined_reviews = pd.concat([existing_reviews, df_reviews_starbucks]).drop_duplicates(subset=['review_id'])\n",
    "        \n",
    "        # Reiniciar índice\n",
    "        combined_reviews.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Guardar el DataFrame combinado\n",
    "        combined_reviews.to_parquet(final_output_file)\n",
    "        print(\"La data se actualizó.\")\n",
    "    else:\n",
    "        # Renombrar el archivo temporal\n",
    "        os.rename(temp_output_file, final_output_file)\n",
    "        print(f\"El archivo se guardó como {final_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo se guardó como ../gcp/reviews_starbucks.parquet\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la función\n",
    "process_reviews_starbucks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL de la Data Review Dunkin Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def process_reviews_dunkin():\n",
    "    # Directorios y archivos\n",
    "    review_file_path = '../data/reviews.json'\n",
    "    business_file_path = '../gcp/business_dunkin.parquet'\n",
    "    output_dir = '../gcp/'\n",
    "    output_file_base = 'reviews_dunkin'\n",
    "    output_file_ext = '.parquet'\n",
    "    \n",
    "    # Leer el archivo reviews.json entero\n",
    "    with open(review_file_path, 'r') as f:\n",
    "        reviews = json.load(f)\n",
    "    df_reviews = pd.DataFrame(reviews)\n",
    "    \n",
    "    # Leer el archivo business_dunkin.parquet\n",
    "    df_business = pd.read_parquet(business_file_path)\n",
    "    \n",
    "    # Verificar que las columnas necesarias existan\n",
    "    if 'business_id' not in df_reviews.columns:\n",
    "        raise KeyError(\"La columna 'business_id' no se encuentra en el archivo reviews.json\")\n",
    "    if 'business_id' not in df_business.columns:\n",
    "        raise KeyError(\"La columna 'business_id' no se encuentra en el archivo business_dunkin.parquet\")\n",
    "    \n",
    "    # Cruzar ambos archivos por la columna business_id\n",
    "    df_reviews_dunkin = df_reviews[df_reviews['business_id'].isin(df_business['business_id'])]\n",
    "    \n",
    "    # Verificar que la columna 'date' exista\n",
    "    if 'date' not in df_reviews_dunkin.columns:\n",
    "        raise KeyError(\"La columna 'date' no se encuentra en el DataFrame resultante después del cruce\")\n",
    "    \n",
    "    # Eliminar la hora de la columna 'date', manteniendo el tipo datetime\n",
    "    df_reviews_dunkin['date'] = pd.to_datetime(df_reviews_dunkin['date']).dt.normalize()\n",
    "    \n",
    "    # Reiniciar el índice del DataFrame\n",
    "    df_reviews_dunkin.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Convertir a parquet sin guardar aún\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    temp_output_file = os.path.join(output_dir, f\"{output_file_base}_{current_date}{output_file_ext}\")\n",
    "    df_reviews_dunkin.to_parquet(temp_output_file)\n",
    "    \n",
    "    final_output_file = os.path.join(output_dir, f\"{output_file_base}{output_file_ext}\")\n",
    "    \n",
    "    # Si el archivo reviews_dunkin.parquet ya existe\n",
    "    if os.path.exists(final_output_file):\n",
    "        existing_reviews = pd.read_parquet(final_output_file)\n",
    "        \n",
    "        # Combinar DataFrames y eliminar duplicados\n",
    "        combined_reviews = pd.concat([existing_reviews, df_reviews_dunkin]).drop_duplicates(subset=['review_id'])\n",
    "        \n",
    "        # Reiniciar índice\n",
    "        combined_reviews.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Guardar el DataFrame combinado\n",
    "        combined_reviews.to_parquet(final_output_file)\n",
    "        print(\"La data se actualizó.\")\n",
    "    else:\n",
    "        # Renombrar el archivo temporal\n",
    "        os.rename(temp_output_file, final_output_file)\n",
    "        print(f\"El archivo se guardó como {final_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo se guardó como ../gcp/reviews_dunkin.parquet\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la función\n",
    "process_reviews_dunkin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL de la Data User Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def process_user_data():\n",
    "    # Definir tamaños de los lotes\n",
    "    batch_size = 10000  # Ajustar según la capacidad de memoria disponible\n",
    "\n",
    "    # Cargar los archivos de reviews\n",
    "    reviews_dunkin_df = pd.read_parquet('../gcp/reviews_dunkin.parquet')\n",
    "    reviews_starbucks_df = pd.read_parquet('../gcp/reviews_starbucks.parquet')\n",
    "\n",
    "    # Obtener los user_id únicos de los archivos de reviews\n",
    "    dunkin_user_ids = reviews_dunkin_df['user_id'].unique()\n",
    "    starbucks_user_ids = reviews_starbucks_df['user_id'].unique()\n",
    "\n",
    "    # Crear un set de user_ids únicos de ambos archivos de reviews\n",
    "    combined_user_ids = set(dunkin_user_ids).union(set(starbucks_user_ids))\n",
    "\n",
    "    # Inicializar el archivo Parquet\n",
    "    user_parquet_file = pq.ParquetFile('../data/user.parquet')\n",
    "\n",
    "    # Inicializar un writer para el archivo temporal\n",
    "    with pd.HDFStore('temp_user.h5', mode='w') as store:\n",
    "        max_lengths = {}\n",
    "\n",
    "        # Procesar el archivo user.parquet en lotes\n",
    "        for batch in user_parquet_file.iter_batches(batch_size):\n",
    "            # Convertir el batch en DataFrame\n",
    "            chunk = batch.to_pandas()\n",
    "\n",
    "            # Filtrar el chunk actual\n",
    "            filtered_chunk = chunk[chunk['user_id'].isin(combined_user_ids)]\n",
    "\n",
    "            # Obtener la longitud máxima de cada columna string en el chunk\n",
    "            for col in filtered_chunk.select_dtypes(include='object').columns:\n",
    "                max_len = filtered_chunk[col].str.len().max()\n",
    "                if col in max_lengths:\n",
    "                    max_lengths[col] = max(max_lengths[col], max_len)\n",
    "                else:\n",
    "                    max_lengths[col] = max_len\n",
    "\n",
    "        # Asegurarse de que cada longitud máxima sea al menos 20 (o un valor adecuado)\n",
    "        min_itemsize = {col: max(20, int(max_len) + 1) if pd.notna(max_len) else 20 for col, max_len in max_lengths.items()}\n",
    "\n",
    "        # Reinicializar la iteración sobre los lotes\n",
    "        for batch in user_parquet_file.iter_batches(batch_size):\n",
    "            # Convertir el batch en DataFrame\n",
    "            chunk = batch.to_pandas()\n",
    "\n",
    "            # Filtrar el chunk actual\n",
    "            filtered_chunk = chunk[chunk['user_id'].isin(combined_user_ids)]\n",
    "\n",
    "            # Escribir el chunk filtrado en el archivo temporal\n",
    "            store.append('filtered_user', filtered_chunk, data_columns=True, index=False, min_itemsize=min_itemsize)\n",
    "\n",
    "    # Cargar el archivo temporal y guardarlo como user.parquet en el directorio ..//gcp//\n",
    "    with pd.HDFStore('temp_user.h5', mode='r') as store:\n",
    "        filtered_user_df = store['filtered_user']\n",
    "\n",
    "    # Eliminar las columnas yelping_since, elite, friends, fans\n",
    "    filtered_user_df.drop(columns=['yelping_since', 'elite', 'friends', 'fans'], inplace=True)\n",
    "\n",
    "    # Resetear el índice\n",
    "    filtered_user_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Nombre del archivo de salida\n",
    "    output_file_path = f'../gcp/user_{today_str}.parquet'\n",
    "\n",
    "    # Guardar el resultado en el directorio ..//gcp// como user_fecha_actual.parquet\n",
    "    filtered_user_df.to_parquet(output_file_path)\n",
    "\n",
    "    # Verificar si existe el archivo user.parquet en el directorio ..//gcp//\n",
    "    existing_user_file_path = '../gcp/user.parquet'\n",
    "    if not os.path.exists(existing_user_file_path):\n",
    "        # Si no existe, renombrar el archivo actual\n",
    "        os.rename(output_file_path, existing_user_file_path)\n",
    "    else:\n",
    "        # Si existe, cruzar el archivo nuevo con user.parquet\n",
    "        existing_user_df = pd.read_parquet(existing_user_file_path)\n",
    "\n",
    "        # Filtrar los nuevos usuarios que no están en el archivo existente\n",
    "        new_users_df = filtered_user_df[~filtered_user_df['user_id'].isin(existing_user_df['user_id'])]\n",
    "\n",
    "        # Combinar los datos existentes con los nuevos usuarios\n",
    "        combined_user_df = pd.concat([existing_user_df, new_users_df], ignore_index=True)\n",
    "\n",
    "        # Guardar el archivo combinado\n",
    "        combined_user_df.to_parquet(existing_user_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamar a la función para procesar los datos\n",
    "process_user_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL de la Data Financiera Yahoo (Starbucks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "def update_stock_data(ticker='SBUX', start_date='2016-01-01', end_date='2024-01-01', file_path='..//gcp//starbucks_nasdaq_data.parquet'):\n",
    "    # Descargar datos de Yahoo Finance\n",
    "    new_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "    # Verificar si el archivo Parquet ya existe\n",
    "    if os.path.exists(file_path):\n",
    "        # Leer los datos existentes\n",
    "        existing_data = pd.read_parquet(file_path)\n",
    "\n",
    "        # Concatenar los datos nuevos con los existentes\n",
    "        updated_data = pd.concat([existing_data, new_data])\n",
    "\n",
    "        # Eliminar duplicados\n",
    "        updated_data = updated_data[~updated_data.index.duplicated(keep='last')]\n",
    "    else:\n",
    "        # Si el archivo no existe, usar solo los datos nuevos\n",
    "        updated_data = new_data\n",
    "\n",
    "    # Guardar los datos actualizados en el archivo Parquet\n",
    "    updated_data.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Llamar a la función para actualizar los datos\n",
    "update_stock_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "\n",
    "def job():\n",
    "    update_stock_data()\n",
    "\n",
    "# Programar la tarea para que se ejecute cada semana\n",
    "schedule.every().week.do(job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrucciones para ejecutar el script automáticamente:\n",
    "Guarda el código de la función update_stock_data y el script de programación en un archivo Python, por ejemplo, update_stock_data.py.\n",
    "Ejecuta este archivo en tu sistema utilizando un programa como cron en Linux o el Programador de Tareas en Windows.\n",
    "Esto hará que la función update_stock_data se ejecute automáticamente cada semana, actualizando los datos financieros y guardando solo los nuevos en el archivo Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>58.770000</td>\n",
       "      <td>58.830002</td>\n",
       "      <td>57.599998</td>\n",
       "      <td>58.259998</td>\n",
       "      <td>49.182022</td>\n",
       "      <td>13521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>58.790001</td>\n",
       "      <td>58.790001</td>\n",
       "      <td>57.980000</td>\n",
       "      <td>58.650002</td>\n",
       "      <td>49.511272</td>\n",
       "      <td>9617800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>57.700001</td>\n",
       "      <td>58.529999</td>\n",
       "      <td>57.639999</td>\n",
       "      <td>58.130001</td>\n",
       "      <td>49.072292</td>\n",
       "      <td>8266300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>56.880001</td>\n",
       "      <td>57.910000</td>\n",
       "      <td>56.160000</td>\n",
       "      <td>56.689999</td>\n",
       "      <td>47.856663</td>\n",
       "      <td>11140900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>57.410000</td>\n",
       "      <td>57.730000</td>\n",
       "      <td>56.529999</td>\n",
       "      <td>56.630001</td>\n",
       "      <td>47.806019</td>\n",
       "      <td>10427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-22</th>\n",
       "      <td>94.889999</td>\n",
       "      <td>95.830002</td>\n",
       "      <td>94.339996</td>\n",
       "      <td>95.279999</td>\n",
       "      <td>93.997032</td>\n",
       "      <td>6360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26</th>\n",
       "      <td>95.339996</td>\n",
       "      <td>95.690002</td>\n",
       "      <td>94.959999</td>\n",
       "      <td>95.669998</td>\n",
       "      <td>94.381783</td>\n",
       "      <td>3709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>95.690002</td>\n",
       "      <td>95.779999</td>\n",
       "      <td>94.790001</td>\n",
       "      <td>95.290001</td>\n",
       "      <td>94.006905</td>\n",
       "      <td>4959300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>95.500000</td>\n",
       "      <td>95.970001</td>\n",
       "      <td>95.110001</td>\n",
       "      <td>95.930000</td>\n",
       "      <td>94.638283</td>\n",
       "      <td>4518300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>95.879997</td>\n",
       "      <td>96.349998</td>\n",
       "      <td>95.349998</td>\n",
       "      <td>96.010002</td>\n",
       "      <td>94.717209</td>\n",
       "      <td>6134000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2012 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume\n",
       "Date                                                                       \n",
       "2016-01-04  58.770000  58.830002  57.599998  58.259998  49.182022  13521500\n",
       "2016-01-05  58.790001  58.790001  57.980000  58.650002  49.511272   9617800\n",
       "2016-01-06  57.700001  58.529999  57.639999  58.130001  49.072292   8266300\n",
       "2016-01-07  56.880001  57.910000  56.160000  56.689999  47.856663  11140900\n",
       "2016-01-08  57.410000  57.730000  56.529999  56.630001  47.806019  10427000\n",
       "...               ...        ...        ...        ...        ...       ...\n",
       "2023-12-22  94.889999  95.830002  94.339996  95.279999  93.997032   6360400\n",
       "2023-12-26  95.339996  95.690002  94.959999  95.669998  94.381783   3709500\n",
       "2023-12-27  95.690002  95.779999  94.790001  95.290001  94.006905   4959300\n",
       "2023-12-28  95.500000  95.970001  95.110001  95.930000  94.638283   4518300\n",
       "2023-12-29  95.879997  96.349998  95.349998  96.010002  94.717209   6134000\n",
       "\n",
       "[2012 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_starbucks_nasdaq_data = pd.read_parquet('..//gcp//starbucks_nasdaq_data.parquet')\n",
    "df_starbucks_nasdaq_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
